{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "c6bda677-3ce4-4ddb-b91a-72d3e00671f7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "6329d4ee-b306-49bd-83c3-2a87c3f2d6a7"
   },
   "outputs": [],
   "source": [
    "#1. Making dataframe (dataset) by using Pandas\n",
    "df = pd.read_csv('Language Detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "bd8cf48a-7ef5-48ea-bb2e-8e08ebdb20a2"
   },
   "outputs": [],
   "source": [
    "#2. Defining a function to remove all the symbols from any data put it.\n",
    "def remove_pun(text):\n",
    "    for pun in string.punctuation:\n",
    "        text = text.replace(pun,\"\")\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "azdata_cell_guid": "3a6f3626-867f-4eaa-af35-2044a3d5666e"
   },
   "outputs": [],
   "source": [
    "#3. Applying the function on the dataset to remove all the symbols.\n",
    "df['Text'] = df['Text'].apply(remove_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "azdata_cell_guid": "b094d395-2c54-4f5c-a365-1463658de480"
   },
   "outputs": [],
   "source": [
    "#Creating variables for training and testing. \"Text\" is the column which will be used for learning and \"Language\" is the column which has name of the languages in it.\n",
    "x = df['Text']\n",
    "y = df['Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "azdata_cell_guid": "c9c20541-2bbc-4cc8-83a1-a7e4fe50fc85"
   },
   "outputs": [],
   "source": [
    "# 3. Splitting datasets into training and testing sets by using sklearn.model_selection (train_test_split) upto 20% for testing set. 4. \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "azdata_cell_guid": "2608c971-3f32-494a-8552-bf36db76441d"
   },
   "outputs": [],
   "source": [
    "#4. Vectorising the text string so that machine learning could understand the data being put in.\n",
    "from sklearn import feature_extraction\n",
    "vec = feature_extraction.text.TfidfVectorizer(ngram_range=(1,2),analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "f7ad4703-a6ab-4c2a-8a91-215e35ec1ae3"
   },
   "outputs": [],
   "source": [
    "#5. Selecting the algorithm to train the model.\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "azdata_cell_guid": "b24e1a89-bfb9-40dc-a6c2-ed52a8c88a78"
   },
   "outputs": [],
   "source": [
    "#6. Pipelining the variables vec and clf as vectorizer and linear model. So that every time data put in will have to go through the pipeline.\n",
    "from sklearn import pipeline\n",
    "nlp_model = pipeline.Pipeline([('vec',vec),('clf',linear_model.LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "azdata_cell_guid": "b6287935-aa49-44f3-8d4a-08981fab4ea8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 2))),\n",
       "                ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. Training the model.\n",
    "nlp_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "azdata_cell_guid": "8b624d98-da23-46f0-8001-4e0c41dcebff"
   },
   "outputs": [],
   "source": [
    "#8. This variable is only created to check the accuracy in the metric.\n",
    "model_prediction = nlp_model.predict(x_test) #x_test are the text column's 20% that is left for model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "azdata_cell_guid": "49d32697-96a5-4072-a03d-51090cab8f35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777562862669246"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9. To check the accuracy, we are testing predicted vs actual y_test values.\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test,model_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "azdata_cell_guid": "6b110dda-c753-49c2-8652-92567b7c15a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'app.py',\n",
       " 'Language Detection.csv',\n",
       " 'Language Detection.ipynb',\n",
       " 'model.pckl',\n",
       " 'Steps for building Language Detector.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we are saving the model as model.pckl to be used later on and also os.listdir() to check if it is saved.\n",
    "import pickle\n",
    "import os\n",
    "new_file = open('model.pckl','wb')\n",
    "pickle.dump(nlp_model,new_file)\n",
    "new_file.close()\n",
    "os.listdir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
